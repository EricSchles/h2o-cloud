{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's first configure the Spark cluster\n",
    "<b>Make sure that:<br> </b>\n",
    "1. You are adding the right Sparkling Water version\n",
    "> For Spark 2.0 -> use the correct Sparkling Water jar file \"sparkling-water-assembly-2-0-all.jar\"<br>\n",
    "\n",
    "2. Set the driver and executor memory to a max of 75% (see [here](http://spark.apache.org/docs/latest/hardware-provisioning.html)) of the RAM of the worker VM types selected on cluster creation\n",
    ">The default worker VM type is D4_v2 (28G RAM, 8 cores), so 75% of RAM = 21G<br>\n",
    ">For information on all available VM sizes click [here](https://azure.microsoft.com/en-us/documentation/articles/virtual-machines-windows-sizes/#dv2-series) \n",
    "3. Set numExecutors to the amount of workers created in the clusters.\n",
    "<b>Note that the \"H2O cluster total nodes has is N-1 worker nodes. This is because the spark deploy mode is yarn-cluster</b>\n",
    ">The default number of worker nodes is 3<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%configure -f\n",
    "{\n",
    "    \"conf\":{\n",
    "        \"spark.ext.h2o.announce.rest.url\": \"http://@@IPADDRESS@@:5000/flows\",\n",
    "        \"spark.jars\":\"wasb:///H2O-Sparkling-Water-files/sparkling-water-assembly-2-0-all.jar\",\n",
    "        \"spark.submit.pyFiles\":\"wasb:///H2O-Sparkling-Water-files/pySparkling-2.0.egg\",\n",
    "        \"spark.locality.wait\":\"3000\",\n",
    "        \"spark.scheduler.minRegisteredResourcesRatio\":\"1\",\n",
    "        \"spark.task.maxFailures\":\"1\",\n",
    "        \"spark.yarn.am.extraJavaOption\":\"-XX:MaxPermSize=384m\",\n",
    "        \"spark.yarn.max.executor.failures\":\"1\",\n",
    "        \"maximizeResourceAllocation\": \"true\"\n",
    "    },\n",
    "    \"driverMemory\":\"21G\",\n",
    "    \"executorMemory\":\"21G\",\n",
    "    \"numExecutors\":3\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis with PySparkling\n",
    "The Amazon Fine Food Reviews dataset consists of 568,454 food reviews Amazon users left up to October 2012.\n",
    "\n",
    "> This data was originally published on SNAP as part of the paper: J. McAuley and J. Leskovec. _From amateurs to connoisseurs: modeling the evolution of user expertise through online reviews_. WWW, 2013.\n",
    "\n",
    "https://www.kaggle.com/snap/amazon-fine-food-reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import pysparkling, h2o\n",
    "import os\n",
    "os.environ[\"PYTHON_EGG_CACHE\"] = \"~/\"\n",
    "\n",
    "h2o_context = pysparkling.H2OContext.getOrCreate(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### H2O FLOW can be found at @@FLOWURL@@"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data into H2OFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This is just helper function returning the path to public data file Reviews.csv ~ 300MB size\n",
    "def _locate(example_name): \n",
    "    return \"https://h2ostore.blob.core.windows.net/examples/\" + example_name \n",
    "\n",
    "DATASET = 'Reviews.csv'\n",
    "\n",
    "# Add files to Spark Cluster\n",
    "\n",
    "sc.addFile(_locate(DATASET))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# And import them into H2O\n",
    "from pyspark import SparkFiles\n",
    "\n",
    "# Since we have already loaded files into spark, we have to use h2o.upload_file instead of \n",
    "# h2o.import_file since h2o.import_file expects cluster-relative path (ie. the file on this\n",
    "# path can be accessed from all the machines on the cluster) but SparkFiles.get(..) already\n",
    "# give us relative path to the file on a current node which h2o.upload_file can handle ( it\n",
    "# uploads file located on current node and distributes it to the H2O cluster)\n",
    "\n",
    "reviews_hf = h2o.upload_file(SparkFiles.get(DATASET))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reviews_hf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Data munge data with H2O API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "selected_columns = [ \"Score\", \"Time\", \"Summary\", \"HelpfulnessNumerator\", \"HelpfulnessDenominator\" ]\n",
    "reviews_hf = reviews_hf[selected_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reviews_hf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Refine `Time` Column into Year/Month/Day/DayOfWeek/Hour columns\n",
    "In this case the `Time` column contains number of seconds from epoch. We translate it into several new columns to help algorithms to pick right pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set time zone to UTC for date manipulation\n",
    "h2o.set_timezone(\"Etc/UTC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def refine_time_column(data_hf, column_name):\n",
    "    data_hf[column_name] = data_hf[column_name] * 1000 # Transformation to microsecond since required by H2O API\n",
    "    data_hf[\"Day\"] = data_hf[column_name].day()\n",
    "    data_hf[\"Month\"] = data_hf[column_name].month()\n",
    "    data_hf[\"Year\"] = data_hf[column_name].year()\n",
    "    data_hf[\"DayOfWeek\"] = data_hf[column_name].dayOfWeek()\n",
    "    data_hf[\"Hour\"] = data_hf[column_name].hour()\n",
    "    \n",
    "refine_time_column(reviews_hf, \"Time\")\n",
    "reviews_hf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Munge with Spark API\n",
    "We can combine H2O data munging capabilities with Spark API\n",
    "\n",
    "### Publish H2O Frame as Spark DataFrame\n",
    "\n",
    "The created H2OContext exposes the method `as_spark_frame` which publishes an H2OFrame as Spark DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reviews_df = h2o_context.as_spark_frame(reviews_hf)\n",
    "reviews_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# HERE is where we save the dataframe to a Hive Table\n",
    "#reviews_df.saveAsTable(\"reviewstable\") #Spark 2.0\n",
    "\n",
    "sqlContext.registerDataFrameAsTable(reviews_df, \"reviewstabletemp\")\n",
    "sqlContext.sql(\"create table reviewstable as select * from reviewstabletemp\");\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%sql \n",
    "show tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark DataFrame API\n",
    "\n",
    "From this point we can run any Spark data munging operations including SQL.\n",
    "We can still publish the result as H2OFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "avgScorePerYear = reviews_df.groupBy(\"Year\").agg({\"Score\" : \"avg\", \"*\": \"count\"}).orderBy(\"Year\")\n",
    "avgScorePerYear.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Another df save to HIVE\n",
    "sqlContext.registerDataFrameAsTable(avgScorePerYear, \"avgscoretabletemp\")\n",
    "sqlContext.sql(\"create table avgscoretable as select * from avgscoretabletemp\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can query the hive table and output the results on a pandas dataframe (using the -o option)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%sql -q -n 500 -o query1\n",
    "select * from avgscoretabletemp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### visualize the results directly in Python Notebook..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%local\n",
    "%matplotlib inline\n",
    "\n",
    "query1.plot.bar(x=\"Year\", y = \"count(1)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data for modeling\n",
    "The idea is to model sentiment based on `Score` of review, `Summary` and time when the review was performed. In this case we skip all neutral reviews, but focus on positive/negative scores.\n",
    "\n",
    "Steps:\n",
    "\n",
    "  1. Select columns Score, Month, Day, DayOfWeek, Summary\n",
    "  2. Define UDF to transform score (0..5) to binary positive/negative\n",
    "  3. Use TF-IDF to vectorize summary column\n",
    "\n",
    "#### Transform the `Score` column into binary feature\n",
    "\n",
    "The score contains value (0, 5), however we are just interested in binary value - positive/negative review. We ignore neutral reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import UserDefinedFunction\n",
    "\n",
    "def to_binary_score(col):\n",
    "    if col < 3:\n",
    "        return \"negative\"\n",
    "    else:\n",
    "        return \"positive\"\n",
    "udf_to_binary_score = UserDefinedFunction(to_binary_score, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reviews_df = reviews_df.withColumn(\"Score\", udf_to_binary_score(\"Score\"))\n",
    "reviews_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transforming textual data into numeric representation\n",
    "\n",
    "#### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import *\n",
    "\n",
    "tokenizer = Tokenizer(inputCol=\"Summary\", outputCol=\"tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transform tokens into numeric representation\n",
    "\n",
    "We use Spark `HashingTF` to represent tokens as numeric features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hashingTF = HashingTF()\n",
    "hashingTF.setInputCol(\"tokens\").setOutputCol(\"tf-features\").setNumFeatures(1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build IDF (Inverse Document Frequency) model\n",
    "The model scales a token frequency based on its occurence in a document and full set of documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "idf = IDF()\n",
    "idf.setInputCol(\"tf-features\")\n",
    "idf.setOutputCol(\"idf-features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compose individual transformation into a Spark pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "\n",
    "pipeline = Pipeline(stages = [tokenizer, hashingTF, idf])\n",
    "pipelineModel = pipeline.fit(reviews_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### And transform input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "final_reviews_df = pipelineModel.transform(reviews_df)\n",
    "#final_reviews_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Back to H2O Frame (materialization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "final_columns = [\"Score\", \"HelpfulnessNumerator\", \"HelpfulnessDenominator\", \"Day\", \"Month\", \"Year\", \"DayOfWeek\", \"idf-features\"]\n",
    "final_reviews_hf = h2o_context.as_h2o_frame(final_reviews_df.select(final_columns), \"final_reviews_hf\")\n",
    "#final_reviews_hf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Score and DayOfWeek columns needs to be a factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "final_reviews_hf[\"Score\"] = final_reviews_hf[\"Score\"].asfactor()\n",
    "final_reviews_hf[\"DayOfWeek\"] = final_reviews_hf[\"DayOfWeek\"].asfactor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare training and validation dataset for modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "splits = final_reviews_hf.split_frame(ratios=[0.75], destination_frames=[\"train\", \"valid\"], seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_hf = splits[0]\n",
    "valid_hf = splits[1]\n",
    "#train_hf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memory Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "final_reviews_hf = None\n",
    "reviews_hf = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### List available data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "h2o.ls()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training\n",
    "\n",
    "### Random grid search with explicit stopping criterions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define a hyper space to explore\n",
    "\n",
    "> Please feel free to play with parameters, see documentation in [H2O Python Documentation](http://docs.h2o.ai/h2o/latest-stable/h2o-py/docs/modeling.html#module-h2o.grid.grid_search)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from h2o.grid.grid_search import H2OGridSearch\n",
    "from h2o.estimators.deeplearning import H2ODeepLearningEstimator\n",
    "\n",
    "hyper_params = {'activation' : [\"Rectifier\", \"TanhWithDropout\"], \n",
    "                'hidden' : [ [2,2], [10,10]],\n",
    "                'epochs' : [ 1, 2, 5]\n",
    "               }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define stoping criterions\n",
    "\n",
    "> Modify based on your demands and requirements (time v. accuracy bound search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "search_criteria = {'strategy' : 'RandomDiscrete',\n",
    "                   'max_runtime_secs': 120,\n",
    "                   'stopping_rounds' : 3,\n",
    "                   'stopping_metric' : 'AUC', # AUTO, mse, logloss\n",
    "                   'stopping_tolerance': 1e-2\n",
    "                   }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Launch Random Hyper Search\n",
    "\n",
    "> For more details look into [H2O Deep Learning documentation](http://docs.h2o.ai/h2o/latest-stable/h2o-py/docs/modeling.html#h2odeeplearningestimator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "models_grid = H2OGridSearch(H2ODeepLearningEstimator, hyper_params=hyper_params, search_criteria=search_criteria)\n",
    "models_grid.train(x = train_hf.col_names, y = \"Score\", \\\n",
    "                  training_frame = train_hf, \\\n",
    "                  validation_frame = valid_hf, \\\n",
    "                  variable_importances=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The best model is ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "models_grid.sort_by('auc', False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The best model details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "best_model = h2o.get_model(models_grid.sort_by('auc', False)[0][0])\n",
    "best_model.model_performance(valid_hf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are most important features?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "best_model.varimp(use_pandas=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Congratulations you built your first model using Azure + PySparkling and H2O!!!"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 2
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
